{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Please make sure you have imported the necessary files from the 'Data Sets' folder within the Zip file.\n",
        "\n",
        "**IMPORTANT** To have the code run correctly, please create a folder named 'coinDataSets' and add all the csv files from the respective folder inside the Zip. At the end of the video we submit, I demonstrate this."
      ],
      "metadata": {
        "id": "MNQo9_EN57DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fredapi #need this to use FRED API\n",
        "!pip install pandas_datareader\n",
        "!pip install wordcloud\n",
        "!pip install yfinance\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTc8lEk_q9Z4",
        "outputId": "817fe4f3-9e13-4083-b368-2e31abdc02f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fredapi in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fredapi) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->fredapi) (1.16.0)\n",
            "Requirement already satisfied: pandas_datareader in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pandas_datareader) (4.9.3)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from pandas_datareader) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pandas_datareader) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_datareader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_datareader) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_datareader) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pandas_datareader) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pandas_datareader) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pandas_datareader) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pandas_datareader) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas_datareader) (1.16.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.27)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.3)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.3.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2023.7.22)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.11.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.10)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.4.4)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbytVeEYlL9S",
        "outputId": "6ee4481e-dbbe-496e-b884-fff909ad99ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t---Pythons That Byte Menu---\n",
            "1) How do closing prices correlate with the S&P 500?\n",
            "2) How does the S&P 500 index affect market phases?\n",
            "3) How do the market phases correlate withe the CBOE volatility index?\n",
            "4) What is the correlation between the CPI and S&P 500?\n",
            "5) What is the relationship between the S&P 500 price action and GNP?\n",
            "6) What is the relationship between US Crude Oil FPP and S&P 500?\n",
            "7) Do presidential administration changes drastically affect the S&P 500?\n",
            "8) Historically, what years have the worst yearly change percentage?\n",
            "9) What years were the best and the lowest in terms of the S&P 500’s value?\n",
            "10) Historically, what year had the highest interests rates, and how did that affect the S&P 500?\n",
            "11) Trends in the S&P 500’s performance during the first 100 days of a new presidential administration?\n",
            "12) Is there any correlation between the S&P 500 and GDP? If there is, is it negative or positive?\n",
            "13) How is the S&P 500 affected during times of protest such as the Occupy Wall Street Movement?\n",
            "14) How has the S&P 500 performed in recents times in comparison to how it’s performed historically?\n",
            "15) What relationship is there with unemployment rates in the US and the S&P 500 index?\n",
            "16) Has there been a significant increase in the number of companies listed on the S&P 500 over the years?\n",
            "17) What seem to be relevant to the S&P 500 index prices?\n",
            "18) Do crashes lead to significant changes in S&P prices?\n",
            "0) Exit\n",
            "Enter choice: h\n",
            "Improper input!\n",
            "\n",
            "\t---Pythons That Byte Menu---\n",
            "1) How do closing prices correlate with the S&P 500?\n",
            "2) How does the S&P 500 index affect market phases?\n",
            "3) How do the market phases correlate withe the CBOE volatility index?\n",
            "4) What is the correlation between the CPI and S&P 500?\n",
            "5) What is the relationship between the S&P 500 price action and GNP?\n",
            "6) What is the relationship between US Crude Oil FPP and S&P 500?\n",
            "7) Do presidential administration changes drastically affect the S&P 500?\n",
            "8) Historically, what years have the worst yearly change percentage?\n",
            "9) What years were the best and the lowest in terms of the S&P 500’s value?\n",
            "10) Historically, what year had the highest interests rates, and how did that affect the S&P 500?\n",
            "11) Trends in the S&P 500’s performance during the first 100 days of a new presidential administration?\n",
            "12) Is there any correlation between the S&P 500 and GDP? If there is, is it negative or positive?\n",
            "13) How is the S&P 500 affected during times of protest such as the Occupy Wall Street Movement?\n",
            "14) How has the S&P 500 performed in recents times in comparison to how it’s performed historically?\n",
            "15) What relationship is there with unemployment rates in the US and the S&P 500 index?\n",
            "16) Has there been a significant increase in the number of companies listed on the S&P 500 over the years?\n",
            "17) What seem to be relevant to the S&P 500 index prices?\n",
            "18) Do crashes lead to significant changes in S&P prices?\n",
            "0) Exit\n",
            "Enter choice: hey\n",
            "Improper input!\n",
            "\n",
            "\t---Pythons That Byte Menu---\n",
            "1) How do closing prices correlate with the S&P 500?\n",
            "2) How does the S&P 500 index affect market phases?\n",
            "3) How do the market phases correlate withe the CBOE volatility index?\n",
            "4) What is the correlation between the CPI and S&P 500?\n",
            "5) What is the relationship between the S&P 500 price action and GNP?\n",
            "6) What is the relationship between US Crude Oil FPP and S&P 500?\n",
            "7) Do presidential administration changes drastically affect the S&P 500?\n",
            "8) Historically, what years have the worst yearly change percentage?\n",
            "9) What years were the best and the lowest in terms of the S&P 500’s value?\n",
            "10) Historically, what year had the highest interests rates, and how did that affect the S&P 500?\n",
            "11) Trends in the S&P 500’s performance during the first 100 days of a new presidential administration?\n",
            "12) Is there any correlation between the S&P 500 and GDP? If there is, is it negative or positive?\n",
            "13) How is the S&P 500 affected during times of protest such as the Occupy Wall Street Movement?\n",
            "14) How has the S&P 500 performed in recents times in comparison to how it’s performed historically?\n",
            "15) What relationship is there with unemployment rates in the US and the S&P 500 index?\n",
            "16) Has there been a significant increase in the number of companies listed on the S&P 500 over the years?\n",
            "17) What seem to be relevant to the S&P 500 index prices?\n",
            "18) Do crashes lead to significant changes in S&P prices?\n",
            "0) Exit\n",
            "Enter choice: -1\n",
            "Improper input!\n",
            "\n",
            "\t---Pythons That Byte Menu---\n",
            "1) How do closing prices correlate with the S&P 500?\n",
            "2) How does the S&P 500 index affect market phases?\n",
            "3) How do the market phases correlate withe the CBOE volatility index?\n",
            "4) What is the correlation between the CPI and S&P 500?\n",
            "5) What is the relationship between the S&P 500 price action and GNP?\n",
            "6) What is the relationship between US Crude Oil FPP and S&P 500?\n",
            "7) Do presidential administration changes drastically affect the S&P 500?\n",
            "8) Historically, what years have the worst yearly change percentage?\n",
            "9) What years were the best and the lowest in terms of the S&P 500’s value?\n",
            "10) Historically, what year had the highest interests rates, and how did that affect the S&P 500?\n",
            "11) Trends in the S&P 500’s performance during the first 100 days of a new presidential administration?\n",
            "12) Is there any correlation between the S&P 500 and GDP? If there is, is it negative or positive?\n",
            "13) How is the S&P 500 affected during times of protest such as the Occupy Wall Street Movement?\n",
            "14) How has the S&P 500 performed in recents times in comparison to how it’s performed historically?\n",
            "15) What relationship is there with unemployment rates in the US and the S&P 500 index?\n",
            "16) Has there been a significant increase in the number of companies listed on the S&P 500 over the years?\n",
            "17) What seem to be relevant to the S&P 500 index prices?\n",
            "18) Do crashes lead to significant changes in S&P prices?\n",
            "0) Exit\n",
            "Enter choice: 0\n",
            "Thank you!\n"
          ]
        }
      ],
      "source": [
        "def nick_q1 () :\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # Import files and save as a pandas dataframe\n",
        "  def create_dataframe(file_path):\n",
        "      df = pd.read_csv(file_path)\n",
        "      # Drop columns with no data\n",
        "      df = df.dropna(axis=1, how='all')\n",
        "      return df\n",
        "\n",
        "  # Combine relevant data from two seperate dataframes\n",
        "  def merge_dataframes(df1,df3):\n",
        "      # Parse dates in the correct format for both dataframes\n",
        "      df1['Date'] = pd.to_datetime(df1['Date'], format='%Y-%m-%d')\n",
        "      df3['date'] = pd.to_datetime(df3['date'], format='%d-%b-%y')\n",
        "\n",
        "      # Extract years from date columns in df1 and df3\n",
        "      df1['Year'] = df1['Date'].dt.year\n",
        "      df3['Year'] = df3['date'].dt.year\n",
        "\n",
        "      # Find common years between df1 and df3\n",
        "      common_years = set(df1['Year']).intersection(df3['Year'])\n",
        "\n",
        "      # Filter dataframes to include only rows from common years\n",
        "      df1_common = df1[df1['Year'].isin(common_years)]\n",
        "      df3_common = df3[df3['Year'].isin(common_years)]\n",
        "\n",
        "      # Merge the dataframes based on common years\n",
        "      merged_df = pd.merge(df1_common, df3_common, on='Year', suffixes=('_df1', '_df3'))\n",
        "\n",
        "      return merged_df\n",
        "\n",
        "  # Create a line plot that compares the two data sets.\n",
        "  def plot_data(merged_df):\n",
        "      # Create a figure and axis\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      ax = plt.gca()\n",
        "\n",
        "      # Plot 'close' data\n",
        "      merged_df.plot(kind='line', x='date', y='close', ax=ax, label='Closing Price', color='blue')\n",
        "\n",
        "      # Plot 'SP500' data\n",
        "      merged_df.plot(kind='line', x='Date', y='SP500', ax=ax, label='SP500 Index', color='red')\n",
        "\n",
        "      # Set labels and title\n",
        "      plt.xlabel('Date')\n",
        "      plt.ylabel('S&P 500 Index')\n",
        "      plt.title('Comparison of Closing Price and S&P 500 Index From 1986-2018')\n",
        "\n",
        "      # Show legend\n",
        "      plt.legend()\n",
        "\n",
        "      # Display the plot\n",
        "      plt.show()\n",
        "\n",
        "  # Create variable to hold names of files to import\n",
        "  file1 = 'sp500_index_data.csv'\n",
        "  file3 = 'spx.csv'\n",
        "\n",
        "  # Run create_dataframe funciton to import all files and store as dataframes\n",
        "  df1 = create_dataframe(file1)\n",
        "  df3 = create_dataframe(file3)\n",
        "\n",
        "  # Run merge_dataframes to combine only elements whose dates were in both datasets into a single dataframe\n",
        "  merged_df = merge_dataframes(df1, df3)\n",
        "\n",
        "  # Run plot_data to create and display the line plot comparing the datasets\n",
        "  plot_data(merged_df)\n",
        "\n",
        "def nick_q2() :\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  from matplotlib.lines import Line2D\n",
        "\n",
        "  # Import file and save as a pandas dataframe\n",
        "  def create_dataframe(file_path):\n",
        "      df = pd.read_csv(file_path)\n",
        "      # Drop columns with no data\n",
        "      df = df.dropna(axis=1, how='all')\n",
        "      return df\n",
        "\n",
        "  # Clean up the dataframe to put the data in a more usable state\n",
        "  def clean_dataframes(df1):\n",
        "      # Parse dates in the correct format\n",
        "      df1['Date'] = pd.to_datetime(df1['Date'], format='%Y-%m-%d')\n",
        "\n",
        "      # Extract years from date column in df1\n",
        "      df1['Year'] = df1['Date'].dt.year\n",
        "\n",
        "      # Remove all data prior to 1940\n",
        "      df1 = df1[df1['Year'] >= 1940]\n",
        "\n",
        "      return df1\n",
        "\n",
        "  # Calculate the Market Phase (Bear, Bull, or Neutral), based primarily off of the +/-20% threshold\n",
        "  def calculate_market_phase(clean_df):\n",
        "      # Set 'Date' column as index\n",
        "      clean_df.set_index('Date', inplace=True)\n",
        "\n",
        "      # Resample data on a monthly basis and calculate monthly percentage changes\n",
        "      monthly_df = clean_df.resample('2M').last()\n",
        "      monthly_df['Monthly_Return'] = monthly_df['SP500'].pct_change() * 100\n",
        "\n",
        "      # Initialize variables for consecutive changes\n",
        "      c_bear = []\n",
        "      c_bull = []\n",
        "      bear_total = 0\n",
        "      bull_total = 0\n",
        "\n",
        "      # Determine market phase for each month\n",
        "      monthly_df['Market_Phase'] = 'Neutral'\n",
        "\n",
        "      for index, row in monthly_df.iterrows():\n",
        "\n",
        "          if row[10] > 0:\n",
        "              c_bull.append(index)\n",
        "              bull_total = bull_total + row[10]\n",
        "              if len(c_bear) > 1:\n",
        "                  if bear_total <= -20:\n",
        "                      for date in c_bear:\n",
        "                          monthly_df.at[date, 'Market_Phase'] = 'Bear'\n",
        "                  c_bear = []\n",
        "                  bear_total = 0\n",
        "          elif row[10] < 0:\n",
        "              c_bear.append(index)\n",
        "              bear_total = bear_total + row[10]\n",
        "              if len(c_bull) > 1:\n",
        "                  if bull_total >= 20:\n",
        "                      for date in c_bull:\n",
        "                          monthly_df.at[date, 'Market_Phase'] = 'Bull'\n",
        "                  c_bull = []\n",
        "                  bull_total = 0\n",
        "\n",
        "      # Extract the Market_Phase column into its own list to simplify element alterations\n",
        "      col_market_phase = monthly_df['Market_Phase'].tolist()\n",
        "\n",
        "      # Improve data for plotting by determining which Neutral phases prematurly interupted or ended a Bear or Bull Phase\n",
        "      c_neutral = 0\n",
        "      previous_phase = None\n",
        "      for i in range(len(col_market_phase)):\n",
        "          if col_market_phase[i] == 'Neutral':\n",
        "              c_neutral = c_neutral + 1\n",
        "          else:\n",
        "              if c_neutral > 0 and c_neutral < 10 and previous_phase == col_market_phase[i]:\n",
        "                  col_market_phase[i - c_neutral:i] = [previous_phase] * c_neutral\n",
        "              c_neutral = 0\n",
        "              previous_phase = col_market_phase[i]\n",
        "\n",
        "      # Send the list of market phase elements back to the Market_Phase dataframe column\n",
        "      monthly_df['Market_Phase'] = col_market_phase\n",
        "\n",
        "      # Iterate through dataframe to further weed out any premature phase terminations using a lower threshold to catch specific mistakes\n",
        "      for index, row in monthly_df.iterrows():\n",
        "          if row['Monthly_Return'] > 5 and row['Market_Phase'] == 'Bear':\n",
        "              monthly_df.at[index, 'Market_Phase'] = 'Neutral'\n",
        "          elif row['Monthly_Return'] < -5 and row['Market_Phase'] == 'Bull':\n",
        "              monthly_df.at[index, 'Market_Phase'] = 'Neutral'\n",
        "\n",
        "      return monthly_df\n",
        "\n",
        "  # Create a plot that displays the S&P 500 index and identifies each phase with a different color\n",
        "  def plot_data(result_df):\n",
        "\n",
        "      # Create a color mapt for determing which color to make each segment based on phase\n",
        "      color_map = {'Bull': 'green', 'Neutral': 'blue','Bear': 'red'}\n",
        "\n",
        "      # Create phase label dictionary to use in legend creation\n",
        "      phase_labels = {'Bear': 'Bear Market','Neutral': 'Neutral Market','Bull': 'Bull Market'}\n",
        "\n",
        "\n",
        "      # Create a line chart with a colored segmented line based on market phase\n",
        "      plt.figure(figsize=(10, 6))\n",
        "\n",
        "      # Initialize the current phase and segment as None\n",
        "      current_phase = None\n",
        "      current_segment = None\n",
        "\n",
        "      # Iterate through dataframe and identify where each phase ends so that the line can be terminated and a new line with the proper color can be stitched on\n",
        "      for index, row in result_df.iterrows():\n",
        "          if current_phase is None:\n",
        "              current_phase = row[11]\n",
        "              current_segment = index\n",
        "          elif current_phase != row[11]:\n",
        "              plt.plot(result_df.loc[current_segment:index].index, result_df.loc[current_segment:index]['SP500'], linestyle='-', color=color_map[current_phase])\n",
        "              current_phase = row[11]\n",
        "              current_segment = index\n",
        "\n",
        "      # Plot the last segment outside of the for loop to terminate the plot\n",
        "      if current_phase is not None:\n",
        "          plt.plot(result_df.loc[current_segment:].index, result_df.loc[current_segment:]['SP500'], linestyle='-', color=color_map[current_phase])\n",
        "\n",
        "\n",
        "      # Set parameters for creating the plot to make it look better and properly convey data\n",
        "      plt.title('Change in Market Phase in Relation to S&P 500 Index from 1940 to 2018')\n",
        "      plt.xlabel('Year')\n",
        "      plt.ylabel('S&P 500 Index')\n",
        "      plt.grid(False)\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.tight_layout()\n",
        "\n",
        "      # Create a legend because the built in one did not like the fact that I technically have like a million lines\n",
        "      legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=phase_labels[phase]) for phase, color in color_map.items()]\n",
        "      plt.legend(handles=legend_elements, loc='upper left', fontsize=12)\n",
        "\n",
        "\n",
        "      # Display the plot\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "  # Create variable to hold names of files to import\n",
        "  file1 = 'sp500_index_data.csv'\n",
        "\n",
        "  # Run create_dataframe funciton to import all files and store as dataframes\n",
        "  df1 = create_dataframe(file1)\n",
        "\n",
        "  # Run clean_df to organize the dataframe in a way that it can be used to calculate market phases\n",
        "  clean_df = clean_dataframes(df1)\n",
        "\n",
        "  # Run calculate_market_phase to calculate the % change in S&P 500 index which can be used to roughly determine market phase\n",
        "  result_df = calculate_market_phase(clean_df)\n",
        "\n",
        "  # Create a line plot to most effectively display how the market phase changes correlate with changes in the S&P 500 index over time\n",
        "  plot_data(result_df)\n",
        "\n",
        "def nick_q3() :\n",
        "  import requests\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  from matplotlib.lines import Line2D\n",
        "\n",
        "  # FRED API endpoint for the VIX index\n",
        "  api_url = 'https://api.stlouisfed.org/fred/series/observations'\n",
        "\n",
        "  # API key\n",
        "  api_key = '77b065d6da18e67b78973664ea28508d'\n",
        "\n",
        "  # Import file and save as a pandas dataframe\n",
        "  def create_dataframe(file_path):\n",
        "      df = pd.read_csv(file_path)\n",
        "      # Drop columns with no data\n",
        "      df = df.dropna(axis=1, how='all')\n",
        "      return df\n",
        "\n",
        "  # Clean up the dataframe to put the data in a more usable state\n",
        "  def clean_dataframes(df1):\n",
        "      # Parse dates in the correct format\n",
        "      df1['Date'] = pd.to_datetime(df1['Date'], format='%Y-%m-%d')\n",
        "\n",
        "      # Extract years from date column in df1\n",
        "      df1['Year'] = df1['Date'].dt.year\n",
        "\n",
        "      # Remove all data prior to 1940\n",
        "      df1 = df1[df1['Year'] >= 1940]\n",
        "\n",
        "      return df1\n",
        "\n",
        "  # Calculate the Market Phase (Bear, Bull, or Neutral), based primarily off of the +/-20% threshold\n",
        "  def calculate_market_phase(clean_df):\n",
        "      # Set 'Date' column as index\n",
        "      clean_df.set_index('Date', inplace=True)\n",
        "\n",
        "      # Resample data on a monthly basis and calculate monthly percentage changes\n",
        "      monthly_df = clean_df.resample('2M').last()\n",
        "      monthly_df['Monthly_Return'] = monthly_df['SP500'].pct_change() * 100\n",
        "\n",
        "      # Initialize variables for consecutive changes\n",
        "      c_bear = []\n",
        "      c_bull = []\n",
        "      bear_total = 0\n",
        "      bull_total = 0\n",
        "\n",
        "      # Determine market phase for each month\n",
        "      monthly_df['Market_Phase'] = 'Neutral'\n",
        "\n",
        "      for index, row in monthly_df.iterrows():\n",
        "\n",
        "          if row[10] > 0:\n",
        "              c_bull.append(index)\n",
        "              bull_total = bull_total + row[10]\n",
        "              if len(c_bear) > 1:\n",
        "                  if bear_total <= -20:\n",
        "                      for date in c_bear:\n",
        "                          monthly_df.at[date, 'Market_Phase'] = 'Bear'\n",
        "                  c_bear = []\n",
        "                  bear_total = 0\n",
        "          elif row[10] < 0:\n",
        "              c_bear.append(index)\n",
        "              bear_total = bear_total + row[10]\n",
        "              if len(c_bull) > 1:\n",
        "                  if bull_total >= 20:\n",
        "                      for date in c_bull:\n",
        "                          monthly_df.at[date, 'Market_Phase'] = 'Bull'\n",
        "                  c_bull = []\n",
        "                  bull_total = 0\n",
        "\n",
        "      # Extract the Market_Phase column into its own list to simplify element alterations\n",
        "      col_market_phase = monthly_df['Market_Phase'].tolist()\n",
        "\n",
        "      # Improve data for plotting by determining which Neutral phases prematurly interupted or ended a Bear or Bull Phase\n",
        "      c_neutral = 0\n",
        "      previous_phase = None\n",
        "      for i in range(len(col_market_phase)):\n",
        "          if col_market_phase[i] == 'Neutral':\n",
        "              c_neutral = c_neutral + 1\n",
        "          else:\n",
        "              if c_neutral > 0 and c_neutral < 10 and previous_phase == col_market_phase[i]:\n",
        "                  col_market_phase[i - c_neutral:i] = [previous_phase] * c_neutral\n",
        "              c_neutral = 0\n",
        "              previous_phase = col_market_phase[i]\n",
        "\n",
        "      # Send the list of market phase elements back to the Market_Phase dataframe column\n",
        "      monthly_df['Market_Phase'] = col_market_phase\n",
        "\n",
        "      # Iterate through dataframe to further weed out any premature phase terminations using a lower threshold to catch specific mistakes\n",
        "      for index, row in monthly_df.iterrows():\n",
        "          if row['Monthly_Return'] > 5 and row['Market_Phase'] == 'Bear':\n",
        "              monthly_df.at[index, 'Market_Phase'] = 'Neutral'\n",
        "          elif row['Monthly_Return'] < -5 and row['Market_Phase'] == 'Bull':\n",
        "              monthly_df.at[index, 'Market_Phase'] = 'Neutral'\n",
        "\n",
        "      return monthly_df\n",
        "\n",
        "  # Utilize the FRED API to retrieve the historical CBOE Volatility data from the Federal Reserve\n",
        "  def retrieve_VIX(api_key, api_url):\n",
        "      # Parameters for the API request\n",
        "      params = {\n",
        "\n",
        "          'series_id': 'VIXCLS',\n",
        "          'api_key': api_key,\n",
        "          'file_type': 'json',\n",
        "          'observation_start': '1990-01-02',\n",
        "          'observation_end': '2018-12-31'\n",
        "          }\n",
        "\n",
        "      # Use a try-except block to catch any errors that may occur while fetching data\n",
        "      try:\n",
        "          response = requests.get(api_url, params=params)\n",
        "\n",
        "          if response.status_code == 200:\n",
        "              # Save successful query response using json processing\n",
        "              data = response.json()\n",
        "\n",
        "              # Extract the observations from the federal reserve\n",
        "              observations = data['observations']\n",
        "\n",
        "              # Create a list of dictionaries for DataFrame creation\n",
        "              vix_data = [{'date': obs['date'], 'vix_value': obs['value']} for obs in observations]\n",
        "\n",
        "              # Create a DataFrame from the list of dictionaries\n",
        "              hist_vix = pd.DataFrame(vix_data)\n",
        "          else:\n",
        "              # If the query fails print appropriate response code\n",
        "              print(f'Request failed with status code: {response.status_code}')\n",
        "\n",
        "      # If an Exception is thrown print the error that occured\n",
        "      except Exception as error:\n",
        "          print(f'An error occurred: {error}')\n",
        "      return hist_vix\n",
        "\n",
        "  # Add the market phase data appropriate for the date ranges in the vix_data dataframe\n",
        "  def combine_vix_snp500(vix_data, result_df):\n",
        "\n",
        "      # Filter data down to only the years that both datasets share\n",
        "      result_df = result_df[result_df.index.year >= 1990]\n",
        "\n",
        "      # Extract market phase data into seperate list to process before transfering to vix_data\n",
        "      col_market_phase = result_df['Market_Phase'].tolist()\n",
        "      vix_data = vix_data.assign(Market_Phase='')\n",
        "      vix_data.set_index('date', inplace=True)\n",
        "\n",
        "      # Convert the index to a DatetimeIndex\n",
        "      vix_data.index = pd.to_datetime(vix_data.index)\n",
        "\n",
        "      # Remove null data points that were represented with '.'\n",
        "      vix_data = vix_data[vix_data['vix_value'] != '.']\n",
        "\n",
        "      # Convert vix value data to float variable type so that an average can be calculated for each month\n",
        "      vix_data['vix_value'] = vix_data['vix_value'].astype(float)\n",
        "\n",
        "      # Group by month and calculate the average vix_value for each month\n",
        "      monthly_avg_vix = vix_data.groupby(pd.Grouper(freq='2M'))['vix_value'].mean()\n",
        "\n",
        "      # Reset index to get a DataFrame with where the date is a column\n",
        "      vix_data = monthly_avg_vix.reset_index()\n",
        "\n",
        "      # Remove the last 4 entries from the list because I couldn't get the objects to be the same length\n",
        "      vix_data = vix_data.drop(vix_data.tail(4).index)\n",
        "\n",
        "      # Pass the market phase data into the vix dataframe\n",
        "      vix_data['Market_Phase'] = col_market_phase\n",
        "\n",
        "      # Make sure the date is in the proper format again\n",
        "      vix_data['date'] = pd.to_datetime(vix_data['date'])\n",
        "\n",
        "      return vix_data\n",
        "\n",
        "  # Plot the data using the calcualted average CBOE Volatility data from the Federal Reserve and the market phases calculated using the S&P 500 index\n",
        "  def plot_data(merged_df):\n",
        "      color_map = {'Bull': 'green', 'Neutral': 'blue','Bear': 'red'}\n",
        "      phase_labels = {'Bear': 'Bear Market','Neutral': 'Neutral Market','Bull': 'Bull Market'}\n",
        "\n",
        "      # Create a line chart of VIX data with segmented line for each market phase calculated from S&P500 data\n",
        "      plt.figure(figsize=(10, 6))\n",
        "\n",
        "      # Initialize the current phase and segment as None\n",
        "      current_phase = None\n",
        "      current_segment = None\n",
        "\n",
        "      # Iterate through dataframe and identify where each phase ends so that the line can be terminated and a new line with the proper color can be stitched on\n",
        "      for index, row in merged_df.iterrows():\n",
        "          if current_phase is None:\n",
        "              current_phase = row[2]\n",
        "              current_segment = index\n",
        "          elif current_phase != row[2]:\n",
        "              plt.plot(merged_df.loc[current_segment:index]['date'], merged_df.loc[current_segment:index]['vix_value'], linestyle='-', color=color_map[current_phase])\n",
        "              current_phase = row[2]\n",
        "              current_segment = index\n",
        "\n",
        "      # Plot the last segment\n",
        "      if current_phase is not None:\n",
        "          plt.plot(merged_df.loc[current_segment:]['date'], merged_df.loc[current_segment:]['vix_value'], linestyle='-', color=color_map[current_phase])\n",
        "\n",
        "      # Set parameters for creating the plot to make it look better and properly convey data\n",
        "      plt.title('CBOE Volatility Index and Market Phase Comparison from 1990 to 2018')\n",
        "      plt.xlabel('Year')\n",
        "      plt.ylabel('CBOE Volatility Index')\n",
        "      plt.grid(False)\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.tight_layout()\n",
        "\n",
        "      # Create a legend because the built in one did not like the fact that I technically have like a million lines\n",
        "      legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=phase_labels[phase]) for phase, color in color_map.items()]\n",
        "      plt.legend(handles=legend_elements, loc='upper left', fontsize=12)\n",
        "\n",
        "      # Display the plot\n",
        "      plt.show()\n",
        "\n",
        "  # Create variable to hold names of file to import\n",
        "  file1 = 'sp500_index_data.csv'\n",
        "\n",
        "  # Run create_dataframe funciton to import all files and store as dataframes\n",
        "  df1 = create_dataframe(file1)\n",
        "\n",
        "  # Run clean_df to organize the dataframe in a way that it can be used to calculate market phases\n",
        "  clean_df = clean_dataframes(df1)\n",
        "\n",
        "  # Run calculate_market_phase to calculate the % change in S&P 500 index which can be used to roughly determine market phase\n",
        "  result_df = calculate_market_phase(clean_df)\n",
        "\n",
        "  # Utilize FRED api to retrieve CBOB data from federal reserve and parse into a dataframe\n",
        "  vix_data = retrieve_VIX(api_key, api_url)\n",
        "\n",
        "  # Filter out unrelevant data and combine into a single file for plotting\n",
        "  merged_df = combine_vix_snp500(vix_data, result_df)\n",
        "\n",
        "  # Create a line plot to most effectively display how the market phase changes correlate with the CBOE Volatility data\n",
        "  plot_data(merged_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def noah_q1 () :\n",
        "  #import necessary libraries\n",
        "  import pandas as pd\n",
        "  import regex as re\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import numpy as np\n",
        "\n",
        "  #extract SP500 prices\n",
        "  def convert_lists_dict(averageS500_list, averageCPI_list) :\n",
        "    convertSP500 = list()\n",
        "    convertCPI = list()\n",
        "\n",
        "    for value in averageS500_list:\n",
        "      convertSP500.append(value[1])\n",
        "\n",
        "    for value in averageCPI_list:\n",
        "      convertCPI.append(value[1])\n",
        "\n",
        "    years.append(2017)\n",
        "\n",
        "    return convertSP500, convertCPI\n",
        "\n",
        "  #percent of change in average S&P price from year before\n",
        "  def percents_S500() :\n",
        "    counter = 0\n",
        "    percents_S500 = list()\n",
        "    years = list()\n",
        "    for index in averageS500_list :\n",
        "      try :\n",
        "        counter += 1\n",
        "        hold_before_S500 = averageS500_list[counter - 1] #holds value before true index placement\n",
        "        hold_past_S500 = averageS500_list[counter] #holds value of true index placement\n",
        "\n",
        "        percent_diff_S500 = (hold_past_S500[1] / hold_before_S500[1]) - 1     #calculates % change in price from year before\n",
        "\n",
        "        percents_S500.append(percent_diff_S500)\n",
        "        years.append(int(hold_past_S500[0]))\n",
        "\n",
        "      except IndexError :\n",
        "        continue\n",
        "\n",
        "    return percents_S500, years\n",
        "\n",
        "  #percent of change in average CPI from year before\n",
        "  def percents_CPI() :\n",
        "    counter = 0\n",
        "    percents_CPI = list()\n",
        "    for index in averageCPI_list :\n",
        "      try :\n",
        "        counter += 1\n",
        "        hold_before_CPI = averageCPI_list[counter - 1]  #same structure as SP500 calculation of % change\n",
        "        hold_past_CPI = averageCPI_list[counter]\n",
        "\n",
        "        percent_diff_CPI = (hold_past_CPI[1] / hold_before_CPI[1]) - 1\n",
        "\n",
        "        percents_CPI.append(percent_diff_CPI)\n",
        "\n",
        "      except IndexError :\n",
        "        continue\n",
        "\n",
        "    return percents_CPI\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #load data into pandas\n",
        "  data_df = pd.read_csv('sp500_index_data.csv')\n",
        "\n",
        "  #gather the average change in CPI and average change in S&P\n",
        "  years = len(data_df) / 12 #amount of years included in the df (CPI and SP500 have same length)\n",
        "\n",
        "  averageCPI_list = list()\n",
        "  averageS500_list = list()\n",
        "\n",
        "  x = 0  #our index holder\n",
        "  is_twelve = 1\n",
        "  total_CPI = 0\n",
        "  total_S500 = 0\n",
        "  for line in data_df.index:\n",
        "    if x == 1764 : #once we get to the most recent complete year, break\n",
        "      break\n",
        "\n",
        "\n",
        "    total_CPI += data_df['Consumer Price Index'][x]   #running totals for the years\n",
        "    total_S500 += data_df['SP500'][x]\n",
        "\n",
        "    if is_twelve == 12 :\n",
        "      is_twelve = 0\n",
        "      average_CPI = total_CPI / 12      #find averages for the years\n",
        "      average_S500 = total_S500 / 12\n",
        "\n",
        "      year = data_df['Date'][x]\n",
        "      extraction = r'\\b\\d{4}\\b'     #extract the year\n",
        "      year = re.search(extraction, year).group()\n",
        "\n",
        "\n",
        "      averageCPI_list.append([year, average_CPI])    #storing list of lists with year : values as key-value pairs\n",
        "      averageS500_list.append([year,average_S500])\n",
        "\n",
        "      total_CPI = 0\n",
        "      total_S500 = 0\n",
        "      average_CPI = 0\n",
        "      average_S500 = 0\n",
        "      year = ''\n",
        "\n",
        "\n",
        "    x += 1  #holds our index placement\n",
        "    is_twelve += 1  #tracks when we hit 12 months in the dataset\n",
        "\n",
        "  percents_SP500, years = percents_S500()\n",
        "  percents_CPI = percents_CPI()\n",
        "\n",
        "  start = len(years)\n",
        "  start -= 50   #only access past 50 years\n",
        "  percents_dict = {\n",
        "      'Years' : years[start:],\n",
        "      'SP500' : percents_SP500[start:],\n",
        "      'CPI' : percents_CPI[start:]\n",
        "  }\n",
        "  dataframe_yearly_percent = pd.DataFrame.from_dict(percents_dict)\n",
        "\n",
        "\n",
        "  start = len(years)\n",
        "  start -= 50\n",
        "  SP500_values, CPI_values = convert_lists_dict(averageS500_list, averageCPI_list)\n",
        "  averages_dict = {\n",
        "      'Years' : years[start:],\n",
        "      \"SP500\" : SP500_values[start:],\n",
        "      \"CPI\" : CPI_values[start:]\n",
        "\n",
        "  }\n",
        "\n",
        "  dataframe_yearly_avg = pd.DataFrame.from_dict(averages_dict)\n",
        "\n",
        "\n",
        "  #PLOTTING\n",
        "  plt.figure(figsize = (20,5))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  correlation_percent = dataframe_yearly_percent['CPI'].corr(dataframe_yearly_percent['SP500'])    #find correlation between values, add to title\n",
        "  sns.regplot(data = dataframe_yearly_percent, x = 'CPI', y = 'SP500').set(title=f'Correlation Between CPI & S&P500 Yearly % Change - (Corr. {correlation_percent:.3f})')\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  correlation_avg = dataframe_yearly_avg['CPI'].corr(dataframe_yearly_avg['SP500'])\n",
        "  sns.regplot(data = dataframe_yearly_avg, x = 'CPI', y = 'SP500').set(title=f'Correlation Between CPI & S&P500 Yearly Average Value - (Corr. {correlation_avg:.3f})')\n",
        "\n",
        "\n",
        "\n",
        "  #plot twin y-axis graph with shared x-axis. Helps visualize relationship over time.\n",
        "  ax = dataframe_yearly_percent.plot(x='Years', y='SP500', legend=False)\n",
        "  ax2 = ax.twinx()\n",
        "  dataframe_yearly_percent.plot(x='Years', y='CPI', ax=ax2, legend=False, color='r')\n",
        "  ax.figure.legend()\n",
        "  ax.set_ylabel('SP500')\n",
        "  ax2.set_ylabel('CPI')\n",
        "  plt.title('Percent Change in S&P500 and CPI Over Time')\n",
        "  plt.show()\n",
        "\n",
        "  ax = dataframe_yearly_avg.plot(x='Years', y='SP500', legend=False)\n",
        "  ax2 = ax.twinx()\n",
        "  dataframe_yearly_avg.plot(x='Years', y='CPI', ax=ax2, legend=False, color='r')\n",
        "  ax.figure.legend()\n",
        "  ax.set_ylabel('SP500')\n",
        "  ax2.set_ylabel('CPI')\n",
        "  plt.title('S&P500 and CPI Over Time')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def noah_q2() :\n",
        "  from fredapi import Fred\n",
        "  fred = Fred(api_key='cb46a20f480a8c2bc9649eb1e6b4b93d') #my key access from FRED API\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "  def dataframes (GNP_dict, SP500_dict) :\n",
        "    df_GNP = pd.DataFrame.from_dict(GNP_dict)     #build our new dataframes\n",
        "    df_SP500 = pd.DataFrame.from_dict(SP500_dict)\n",
        "    return df_GNP, df_SP500\n",
        "\n",
        "  GNP = fred.get_series('GNP') #get data from FRED api as a series\n",
        "\n",
        "  SP500 = fred.get_series('SP500')\n",
        "\n",
        "\n",
        "  #take out values, convert to dataframe\n",
        "  years_GNP = GNP.index\n",
        "  start = len(years_GNP) - 40\n",
        "  GNP_values = GNP.values\n",
        "\n",
        "  years_SP500 = SP500.index\n",
        "  daily_SP500 = SP500.values\n",
        "\n",
        "\n",
        "  GNP_dict = {\n",
        "      'Years_GNP' : years_GNP[start:], #first quarter of 2013 to last quarter of 2022. SP500 only has prices for last 10 years, so we match GNP timeline.\n",
        "      'GNP' : GNP_values[start:],\n",
        "  }\n",
        "  SP500_dict = {\n",
        "      'Years_SP500' : years_SP500,\n",
        "      'SP500' : daily_SP500\n",
        "  }\n",
        "\n",
        "  df_GNP, df_SP500 = dataframes(GNP_dict, SP500_dict)\n",
        "\n",
        "  #PLOTTING\n",
        "  #plotting twin y-axis graph with shared x-acis to show relationship over time\n",
        "  plt.figure(figsize = (20,5))\n",
        "\n",
        "  ax = df_SP500.plot(x='Years_SP500', y='SP500', legend = False)\n",
        "  ax2= ax.twinx()\n",
        "  df_GNP.plot(x='Years_GNP', y='GNP', ax=ax2, legend=False, color='r')\n",
        "  ax.figure.legend()\n",
        "  ax2.set_ylabel('GNP (in thousands of billions)')\n",
        "  ax.set_ylabel('S&P (in thousands)')\n",
        "  ax.set_xlabel('Years')\n",
        "  correlation_percent = df_SP500['SP500'].corr(df_GNP['GNP'])   #find correlation between GNP and SP500 over past decade\n",
        "  plt.title(f'S&P500 and GNP Over Time (r: {correlation_percent})')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def noah_q3() :\n",
        "  from fredapi import Fred\n",
        "  key = 'cb46a20f480a8c2bc9649eb1e6b4b93d' #my api key access for FRED API\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  import requests\n",
        "  from bs4 import BeautifulSoup\n",
        "  import urllib.request\n",
        "  import regex as re\n",
        "  import numpy as np\n",
        "  start_date = '2013-01-01' #want data for past decade from FRED API\n",
        "  end_date = '2023-08-08'\n",
        "\n",
        "#get subset dataframe from SP500 JSON\n",
        "  def get_subset (key, start_date, end_date) :\n",
        "    seriesID = 'SP500'\n",
        "    url = f\"https://api.stlouisfed.org/fred/series/observations?series_id={seriesID}&api_key={key}&file_type=json\" + \\\n",
        "          f\"&observation_start={start_date}&observation_end={end_date}&units=lin\"\n",
        "\n",
        "    SP500 = requests.get(url)\n",
        "    SP500_json = SP500.json()  #FRED will return SP500 data in JSON which we convert to a dataframe below\n",
        "\n",
        "    # Convert JSON data to a DataFrame\n",
        "    df = pd.DataFrame(SP500_json['observations'])\n",
        "\n",
        "    # Rename columns for clarity\n",
        "    df.rename(columns={'date': 'Date', 'value': 'Value'}, inplace=True)\n",
        "\n",
        "\n",
        "    #re-arrange data in columnds for plotting purposes. We want dates and values in ascending order from L-R on the plot\n",
        "    df['Date'] = df['Date'].iloc[::-1].reset_index(drop=True)\n",
        "    df['Value'] = df['Value'].iloc[::-1].reset_index(drop=True)\n",
        "    df_subset = df.iloc[::91, :] #creating a subset dataframe with every 91 value. 91 represents the amount of days per quarter in a year\n",
        "\n",
        "    df_subset = df_subset.iloc[::-1].reset_index(drop=True)\n",
        "    df_subset = df_subset.drop(22) #drop a value that was causing error. Value was a '.' instead of a SP500 price\n",
        "    return df_subset\n",
        "\n",
        "\n",
        "  def convert_SP500_float (df_subset) :\n",
        "    #convert SP500 dataframe into legible float\n",
        "    years_SP500 = list()\n",
        "    date_list = df_subset['Date'].tolist()  #extract date column into list\n",
        "    for value in date_list :\n",
        "      pattern = r'^(\\d{4})-\\d{2}-\\d{2}$' #extract years from the date format\n",
        "      match = re.search(pattern, value)\n",
        "      x = match.group(1)\n",
        "      years_SP500.append(int(x))  #add extracted year into years_SP500 list\n",
        "\n",
        "    values_SP500 = list()\n",
        "    value_list = df_subset['Value'].tolist()  #extract SP500 prices into list and convert to float. Append to values_SP500 list\n",
        "    for value in value_list :\n",
        "      x = float(value)\n",
        "      values_SP500.append(int(x))\n",
        "\n",
        "    return years_SP500, values_SP500\n",
        "\n",
        "  #webscrape for the data we need\n",
        "  def webscrape (soup) :\n",
        "    data = {} #holds prices of US Cruse Oil FPP with their decade. Data stored as key : list (decade : prices)\n",
        "\n",
        "    for row in soup.find_all('tr'): #find all tr tags\n",
        "        cells = row.find_all('td') #find rows with td tags\n",
        "        if cells and len(cells) > 1:\n",
        "            year = cells[0].text.strip() #extract year\n",
        "            if year == \"2010's\" or year == \"2020's\": #only extract decades that can be used in correlation with SP500 timeline (past decade)\n",
        "                try:\n",
        "                    prices = [float(cell.text.strip()) if cell.text.strip() != '' else None for cell in cells[1:]] #extract prices with an exception if there is no value\n",
        "                    data[year] = prices\n",
        "                except ValueError:\n",
        "                    pass\n",
        "    return data\n",
        "\n",
        "\n",
        "  #webscrape oil data from web\n",
        "  url = 'https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=f000000__3&f=a'\n",
        "  response = requests.get(url)\n",
        "  mystr = response.content #holds html\n",
        "  soup = BeautifulSoup(mystr, 'html.parser') #use beautiful soup to parse html\n",
        "  data = webscrape(soup)\n",
        "\n",
        "\n",
        "  years = list()\n",
        "  values = list()\n",
        "  year_counter = 2010\n",
        "\n",
        "  #create years to match prices that exist in data{}\n",
        "  for value in data.values() :\n",
        "    for item in value:\n",
        "      years.append(year_counter)\n",
        "      values.append(item)\n",
        "\n",
        "      year_counter += 1\n",
        "\n",
        "  #only use years from past decade (2013-2023)\n",
        "  oil_dict = {\n",
        "      'Years' : years[3:13],\n",
        "      'US FPP' : values[3:13],\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "  oil_df = pd.DataFrame.from_dict(oil_dict)\n",
        "  df_subset = get_subset(key, start_date, end_date)\n",
        "  years_SP500, values_SP500 = convert_SP500_float(df_subset)\n",
        "\n",
        "\n",
        "  SP500_dict = {\n",
        "      'Years' : years_SP500,\n",
        "      'SP500 Prices' : values_SP500\n",
        "  }\n",
        "\n",
        "\n",
        "  df_SP500 = pd.DataFrame.from_dict(SP500_dict)\n",
        "\n",
        "  #PLOTTING\n",
        "  # Plot both dataframes on the same x-axis with a secondary y-axis\n",
        "  plt.figure(figsize=(20,5))\n",
        "\n",
        "  ax = oil_df.plot(x='Years',y='US FPP',legend=False)\n",
        "  ax2=ax.twinx()\n",
        "  df_SP500.plot(x='Years',y='SP500 Prices', ax=ax2, legend=False, color='r')\n",
        "  ax.figure.legend()\n",
        "  ax.set_ylabel('US Crude Oil First Purchase Prices')\n",
        "  ax.set_xlabel('Years')\n",
        "  ax2.set_ylabel('SP500 Prices')\n",
        "  correlation_percent = df_SP500['SP500 Prices'].corr(oil_df['US FPP'])  #correlation between US Crude Oil FPP and SP500 prices over past decade\n",
        "  plt.title(f'US Crude Oil FPP and S&P 500 Over Time\\n(r: {correlation_percent:.3f})')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def siya_q1() :\n",
        "  #Importing for fetching financial data\n",
        "  import yfinance as yf\n",
        "  #Importing for data manipulation and analysis\n",
        "  import pandas as pd\n",
        "  #Importing for data visualization\n",
        "  import matplotlib.pyplot as plt\n",
        "  #Importing data module for economic data\n",
        "  from pandas_datareader import data as pdr\n",
        "  #Importing for creating word clouds\n",
        "  from wordcloud import WordCloud\n",
        "\n",
        "  #Analyzing S&P 500 Performance:\n",
        "  #This program uses historical S&P 500 data and interest rates, then visualizes the performance of the S&P 500 index during different presidential administrations\n",
        "  #The goal is to analyze how various presidential terms have influenced the S&P 500's historical performance and identify visible trends\n",
        "\n",
        "  #FRED API Personal Key\n",
        "  fred_api_key = '8dd835b14e8ddd845fad438c41e2009a'\n",
        "\n",
        "  #Finding S&P 500 data using yfinance\n",
        "  ticker = \"^GSPC\"\n",
        "  start_date = \"1957-01-01\"\n",
        "  end_date = \"2023-01-01\"\n",
        "  #Fetch S&P 500 data within the specified date range\n",
        "  spx_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "  #Finding historical interest rates data\n",
        "  fred_data = pdr.DataReader(\"GS10\", \"fred\", start=start_date, end=end_date, api_key=fred_api_key)\n",
        "  #Extract the GS10 interest rate data\n",
        "  historical_interest_rates_data = fred_data[\"GS10\"]\n",
        "\n",
        "  #Finding the years with the highest interest rates\n",
        "  highest_rate_year = historical_interest_rates_data.idxmax().year\n",
        "\n",
        "  #Finding the corresponding S&P 500 data for the year with the highest interest rate and filtering it\n",
        "  spx_data_highest_rate_year = spx_data[spx_data.index.year == highest_rate_year]\n",
        "\n",
        "  #Presidential administration periods, start and end years\n",
        "  presidential_periods = [\n",
        "      {\"start\": 1981, \"end\": 1988, \"name\": \"Reagan\"},\n",
        "      {\"start\": 1989, \"end\": 1992, \"name\": \"Bush Sr.\"},\n",
        "      {\"start\": 1993, \"end\": 2000, \"name\": \"Clinton\"},\n",
        "      {\"start\": 2001, \"end\": 2008, \"name\": \"Bush Jr.\"},\n",
        "      {\"start\": 2009, \"end\": 2016, \"name\": \"Obama\"},\n",
        "      {\"start\": 2017, \"end\": 2024, \"name\": \"Trump\"},\n",
        "      {\"start\": 2025, \"end\": 2032, \"name\": \"Future President\"}\n",
        "  ]\n",
        "\n",
        "  #Plotting the S&P 500's historical performance during presidential administration periods\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  #Plotting the overall S&P 500 performance\n",
        "  spx_data[\"Adj Close\"].plot(label=\"S&P 500\")\n",
        "\n",
        "  for period in presidential_periods:\n",
        "      start_year = period[\"start\"]\n",
        "      end_year = period[\"end\"]\n",
        "      period_data = spx_data[(spx_data.index.year >= start_year) & (spx_data.index.year <= end_year)]\n",
        "      #Plotting S&P 500 performance during each presidential period\n",
        "      period_data[\"Adj Close\"].plot(label=period[\"name\"])\n",
        "\n",
        "  #Specifying plot's visual aspects\n",
        "  spx_data_highest_rate_year[\"Adj Close\"].plot(marker=\"o\", linestyle=\"\", markersize=8, color=\"red\", label=\"Highest Rate Year\")\n",
        "  plt.title(f\"S&P 500 Performance and Presidential Administrations\")\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Adjusted Close Price\")\n",
        "  plt.legend()\n",
        "  #Printing the visual\n",
        "  plt.show()\n",
        "\n",
        "  #Word Cloud visualization for presidential administrations' impact on the S&P 500\n",
        "\n",
        "  #Creating a text string containing the names of presidential administrations\n",
        "  #This will be used to generate the word cloud\n",
        "  presidential_impact_text = \" \".join([f\"{period['name']} administration\" for period in presidential_periods])\n",
        "\n",
        "  #Creating a WordCloud object with specified settings\n",
        "  wordcloud = WordCloud(\n",
        "      width=800, height=400, background_color='black', colormap='viridis',\n",
        "      contour_color='white', contour_width=1, min_word_length=4,\n",
        "      max_font_size=100, relative_scaling=0.5\n",
        "  ).generate(presidential_impact_text)\n",
        "\n",
        "  #Creating a new figure for the word cloud visualization\n",
        "  plt.figure(figsize=(6, 4))\n",
        "\n",
        "  #Displaying the word cloud image with bilinear interpolation\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "  #Turning off axis labels and ticks\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  #Setting the title for the word cloud visualization\n",
        "  plt.title(\"Word Cloud: Presidential Administrations' Impact on S&P 500\", fontsize=14)\n",
        "\n",
        "  #Adjusting layout for better spacing\n",
        "  plt.tight_layout()\n",
        "\n",
        "  #Displaying the word cloud plot\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def siya_q2() :\n",
        "  #Importing for fetching financial data\n",
        "  import yfinance as yf\n",
        "  #Importing for data manipulation and analysis\n",
        "  import pandas as pd\n",
        "  #Importing for data visualization\n",
        "  import matplotlib.pyplot as plt\n",
        "  #Importing data module for economic data\n",
        "  from pandas_datareader import data as pdr\n",
        "  #Importing for creating word clouds\n",
        "  from wordcloud import WordCloud\n",
        "  #Finding S&P 500 data using yfinance\n",
        "  ticker = \"^GSPC\"\n",
        "  start_date = \"1957-01-01\"\n",
        "  end_date = \"2023-01-01\"\n",
        "  spx_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "  #Calculating the yearly percentage change for the S&P 500\n",
        "  spx_data[\"Yearly Change %\"] = spx_data[\"Adj Close\"].pct_change(periods=1) * 100\n",
        "\n",
        "  #Finding the years with the worst yearly change percentage\n",
        "  worst_years = spx_data[spx_data[\"Yearly Change %\"].notnull()].nsmallest(5, \"Yearly Change %\").index.year\n",
        "\n",
        "  #Plotting the S&P 500's historical performance and mark the worst years using matplotlib\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  spx_data[\"Adj Close\"].plot(label=\"S&P 500\")\n",
        "  plt.scatter(spx_data.loc[spx_data.index.year.isin(worst_years), :].index, spx_data.loc[spx_data.index.year.isin(worst_years), \"Adj Close\"], color=\"orange\", marker=\"v\", label=\"Worst Years\")\n",
        "\n",
        "  #Setting the title of the plot\n",
        "  plt.title(\"S&P 500 Performance and Worst Years by Yearly Change %\")\n",
        "  #Setting the label for the x-axis\n",
        "  plt.xlabel(\"Date\")\n",
        "  #Setting the label for the y-axis\n",
        "  plt.ylabel(\"Adjusted Close Price\")\n",
        "  #Displaying the legend in the plot\n",
        "  plt.legend()\n",
        "  #Printing the visual\n",
        "  plt.show()\n",
        "\n",
        "  #Printing the list of historically worst years by yearly change percentage\n",
        "  print(\"Historically worst years by yearly change percentage:\")\n",
        "\n",
        "  #Looping through each year in the list of worst years\n",
        "  for year in worst_years:\n",
        "      #Printing the year and its yearly change percentage\n",
        "      print(f\"{year}: {spx_data.loc[spx_data.index.year == year, 'Yearly Change %'].values[0]:.2f}%\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "def siya_q3() :\n",
        "  #Importing for fetching financial data\n",
        "  import yfinance as yf\n",
        "  #Importing for data manipulation and analysis\n",
        "  import pandas as pd\n",
        "  #Importing for data visualization\n",
        "  import matplotlib.pyplot as plt\n",
        "  #Importing data module for economic data\n",
        "  from pandas_datareader import data as pdr\n",
        "  #Importing for creating word clouds\n",
        "  from wordcloud import WordCloud\n",
        "  #Finding S&P 500 data using yfinance\n",
        "\n",
        "  #S&P 500 Performance: Best and Lowest Years\n",
        "  #This code uses historical S&P 500 data and identifies the years with the highest and lowest values of the index.\n",
        "  #It then visualizes the performance of the S&P 500 over time, highlighting the best and lowest years, and provides information on these years.\n",
        "\n",
        "  #Finding S&P 500 data using yfinance\n",
        "  ticker = \"^GSPC\"\n",
        "  start_date = \"1957-01-01\"\n",
        "  end_date = \"2023-01-01\"\n",
        "  #S&P 500 data within the specified date range\n",
        "  spx_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "  #Resampling the data to find the years with the highest and lowest values of the S&P 500\n",
        "  spx_yearly = spx_data[\"Adj Close\"].resample('Y').agg({\"Adj Close\": \"last\"})\n",
        "  #Finding the year with the highest S&P 500 value\n",
        "  best_year = spx_yearly[\"Adj Close\"].idxmax().year\n",
        "  #Finding the year with the lowest S&P 500 value\n",
        "  lowest_year = spx_yearly[\"Adj Close\"].idxmin().year\n",
        "\n",
        "  #Plotting the S&P 500's historical performance and mark the best and lowest years\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  spx_data[\"Adj Close\"].plot(label=\"S&P 500\")  # Plot the overall S&P 500 performance\n",
        "  plt.scatter(pd.Timestamp(f'{best_year}-12-31'), spx_yearly.loc[pd.Timestamp(f'{best_year}-12-31'), \"Adj Close\"], color=\"green\", marker=\"^\", label=\"Best Year\")  # Mark the best year with a green triangle marker\n",
        "  plt.scatter(pd.Timestamp(f'{lowest_year}-12-31'), spx_yearly.loc[pd.Timestamp(f'{lowest_year}-12-31'), \"Adj Close\"], color=\"red\", marker=\"v\", label=\"Lowest Year\")  # Mark the lowest year with a red triangle marker\n",
        "\n",
        "  plt.title(\"S&P 500 Performance and Best/Lowest Years\")\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Adjusted Close Price\")\n",
        "  plt.legend()\n",
        "  #Displaying the plot\n",
        "  plt.show()\n",
        "\n",
        "  #Printing information about the best and lowest years\n",
        "  print(f\"The best year for the S&P 500 was: {best_year}\")\n",
        "  print(f\"The lowest year for the S&P 500 was: {lowest_year}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def brady_q1() :\n",
        "  import yfinance as yf\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  from fredapi import Fred\n",
        "\n",
        "  # Fetch S&P 500 data using yfinance\n",
        "  ticker = \"^GSPC\"\n",
        "  start_date = \"1957-01-01\"\n",
        "  end_date = \"2023-01-01\"\n",
        "  spx_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "  # Fetch historical interest rates data using the FRED API\n",
        "  fred = Fred(api_key='767f7894f66f59b2796e04a5525ad91e')\n",
        "  historical_interest_rates_data = fred.get_series(\"DGS10\")\n",
        "\n",
        "  # Find the years with the highest interest rates\n",
        "  highest_rate_year = historical_interest_rates_data.idxmax().year\n",
        "\n",
        "  # Find the corresponding S&P 500 data for the year with the highest interest rate\n",
        "  spx_data_highest_rate_year = spx_data[spx_data.index.year == highest_rate_year]\n",
        "\n",
        "  # Plot the S&P 500's historical performance during the year with the highest interest rate using matplotlib\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  spx_data[\"Adj Close\"].plot(label=\"S&P 500\")\n",
        "  spx_data_highest_rate_year[\"Adj Close\"].plot(marker=\"o\", linestyle=\"\", markersize=8, color=\"red\", label=\"Highest Rate Year\")\n",
        "  plt.title(f\"S&P 500 Performance during {highest_rate_year} (Highest Interest Rate Year)\")\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Adjusted Close Price\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  ####THIS IS THE SAME AS THE CODE ABOVE BUT CHANGED THE YEAR FOR A CLOSER LOOK\n",
        "\n",
        "  # Fetch S&P 500 data using yfinance\n",
        "  ticker = \"^GSPC\"\n",
        "  start_date = \"1980-01-01\"\n",
        "  end_date = \"1990-01-01\"\n",
        "  spx_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "  # Fetch historical interest rates data using the FRED API\n",
        "  fred = Fred(api_key='767f7894f66f59b2796e04a5525ad91e')\n",
        "  historical_interest_rates_data = fred.get_series(\"DGS10\")\n",
        "\n",
        "  # Find the years with the highest interest rates\n",
        "  highest_rate_year = historical_interest_rates_data.idxmax().year\n",
        "\n",
        "  # Find the corresponding S&P 500 data for the year with the highest interest rate\n",
        "  spx_data_highest_rate_year = spx_data[spx_data.index.year == highest_rate_year]\n",
        "\n",
        "  # Plot the S&P 500's historical performance during the year with the highest interest rate using matplotlib\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  spx_data[\"Adj Close\"].plot(label=\"S&P 500\")\n",
        "  spx_data_highest_rate_year[\"Adj Close\"].plot(marker=\"o\", linestyle=\"\", markersize=8, color=\"red\", label=\"Highest Rate Year\")\n",
        "  plt.title(f\"S&P 500 Performance during {highest_rate_year} (Highest Interest Rate Year)\")\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Adjusted Close Price\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def brady_q2() :\n",
        "  import yfinance as yf\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # Function to calculate the start date of each president's term\n",
        "  def calculate_start_dates():\n",
        "      start_dates = pd.to_datetime([\n",
        "          '1974-08-09', '1977-01-20', '1981-01-20', '1985-01-21', '1989-01-20', '1993-01-20', '1997-01-20',\n",
        "          '2001-01-20', '2005-01-20', '2009-01-20', '2013-01-21', '2017-01-20', '2021-01-20'\n",
        "      ])\n",
        "      return start_dates\n",
        "\n",
        "  # Function to plot the performance during the first 100 days of each president's term\n",
        "  def plot_first_100_days_performance(data):\n",
        "      first_days = calculate_start_dates()\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      for i, first_day in enumerate(first_days):\n",
        "          closest_date_index = data.index.get_indexer([first_day])[0]\n",
        "          closest_date = data.index[closest_date_index]\n",
        "          start_index = data.index.get_loc(closest_date)\n",
        "          end_index = start_index + 100\n",
        "          president_data = data.iloc[start_index:end_index]\n",
        "\n",
        "          # Choose a color for each president\n",
        "          colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'tab:orange', 'tab:brown', 'tab:gray', 'tab:purple', 'tab:pink', 'tab:olive']\n",
        "          plt.plot(president_data.index, president_data['Close'], color=colors[i], label=f\"President {i + 1}\")\n",
        "\n",
        "      plt.legend()\n",
        "      plt.xlabel(\"Date\")\n",
        "      plt.ylabel(\"S&P 500 Close Price\")\n",
        "      plt.title(\"S&P 500 Performance during First 100 Days of Each President's Term\")\n",
        "      plt.grid(True)  # Add gridlines to the plot\n",
        "      plt.show()\n",
        "\n",
        "  # Download S&P 500 data\n",
        "  spx_data = yf.download('^GSPC', start='1972-01-01', end='2023-01-01', progress=False)\n",
        "\n",
        "  # Call the function to plot the performance during the first 100 days of each president's term\n",
        "  plot_first_100_days_performance(spx_data)\n",
        "\n",
        "\n",
        "def brady_q3() :\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  import requests\n",
        "  from bs4 import BeautifulSoup\n",
        "  import json\n",
        "\n",
        "  # Read the CSV file into a pandas DataFrame\n",
        "  df = pd.read_csv('sp500_index_data.csv')\n",
        "\n",
        "  # Display the first few rows of the DataFrame\n",
        "  df.isnull().any()\n",
        "  df = df.dropna()\n",
        "  df.isnull().any()\n",
        "  df.shape\n",
        "  df.drop_duplicates(inplace=True)\n",
        "  df.shape\n",
        "  df.columns\n",
        "\n",
        "  #Rename the columns\n",
        "  df.rename(columns={\n",
        "      'Date':'date',\n",
        "      'Dividend': 'dividend',\n",
        "      'Earnings' :'earnings',\n",
        "      'Consumer Price Index': 'consumer_price_index',\n",
        "      'Long Interest Rate': 'long_interest_rate',\n",
        "      'Real Price': 'real_price',\n",
        "      'Real Dividend': 'real_dividend',\n",
        "      'Real Earnings': 'real_earnings'\n",
        "  }, inplace = True)\n",
        "\n",
        "  df.columns\n",
        "\n",
        " # Convert DataFrame to JSON\n",
        "  json_data = df.to_json(orient='records')\n",
        "\n",
        "\n",
        "  # Read the CSV file into a pandas DataFrame\n",
        "  df = pd.read_csv('sp500_index_data.csv')\n",
        "\n",
        "  # Convert 'Date' column to datetime format\n",
        "  df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "  # Set 'Date' column as the index\n",
        "  df.set_index('Date', inplace=True)\n",
        "\n",
        "  # Calculate the yearly average of 'SP500' column\n",
        "  yearly_average = df['SP500'].resample('Y').mean()\n",
        "\n",
        "\n",
        "\n",
        "  # URL of the Wikipedia page\n",
        "  url = \"https://en.wikipedia.org/wiki/Economy_of_the_United_States#Data\"\n",
        "\n",
        "  table_class = 'wikitable sortable jquery-tablesorter'\n",
        "  # Send an HTTP GET request to the URL and get the content\n",
        "  response = requests.get(url)\n",
        "  html_content = response.content\n",
        "\n",
        "  # Parse the HTML content using BeautifulSoup\n",
        "  # parse data from the html into a beautifulsoup object\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  indiatable=soup.find('table',{'class':\"wikitable\"})\n",
        "  #print(indiatable)\n",
        "\n",
        "  df=pd.read_html(str(indiatable))\n",
        "  # convert list to dataframe\n",
        "  df=pd.DataFrame(df[0])\n",
        "\n",
        "  df.to_csv('wiki_gdp_data.csv', sep=',', index=False, encoding='utf-8')\n",
        "\n",
        "  #clean the data for easy viewing\n",
        "  json_string = df.to_json('output.json', orient='records', lines = False, indent =4)\n",
        "\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "  df = pd.read_csv('sp500_index_data.csv')\n",
        "\n",
        "  # Convert 'Date' column to datetime format\n",
        "  df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "  # Set 'Date' column as the index\n",
        "  df.set_index('Date', inplace=True)\n",
        "\n",
        "  # Calculate the yearly average of 'SP500' column\n",
        "  yearly_average = df['SP500'].resample('Y').mean()\n",
        "\n",
        "  # Convert the Python dictionary to a pandas DataFrame\n",
        "  gdp_df = pd.read_json('output.json')\n",
        "\n",
        "  # Merge the GDP DataFrame with the S&P 500 DataFrame based on the 'Year' column\n",
        "  merged_df = pd.merge(gdp_df, df, left_on='Year', right_on=df.index.year)\n",
        "\n",
        "  # Calculate the correlation between the 'GDP (in Bil. US$PPP)' and 'SP500' columns\n",
        "  correlation = merged_df['GDP (in Bil. US$PPP)'].corr(merged_df['SP500'])\n",
        "\n",
        "  # Create a scatter plot to visualize the correlation\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.scatter(merged_df['GDP (in Bil. US$PPP)'], merged_df['SP500'])\n",
        "  plt.xlabel('GDP (in Bil. US$PPP)')\n",
        "  plt.ylabel('S&P 500')\n",
        "  plt.title(f'Correlation between GDP and S&P 500 (Correlation: {correlation:.2f})')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def lizette_q1() :\n",
        "  import matplotlib.pyplot as plt\n",
        "  import json\n",
        "  from wordcloud import WordCloud\n",
        "  from bs4 import BeautifulSoup\n",
        "  from newspaper import Article\n",
        "  import pandas as pd\n",
        "\n",
        "\n",
        "  df = pd.read_csv('spx.csv')\n",
        "  df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "  start_date = pd.to_datetime('2011-09-01')\n",
        "  end_date = pd.to_datetime('2011-10-31')\n",
        "  protest = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
        "\n",
        "  start_date_jan = pd.to_datetime('2011-01-01')\n",
        "  end_date_aug = pd.to_datetime('2011-08-31')\n",
        "  preProtest = df[(df['date'] >= start_date_jan) & (df['date'] <= end_date_aug)]\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "\n",
        "  plt.plot(protest['date'], protest['close'], label='During Occupy Wall Street Movement', color='blue')\n",
        "  plt.plot(preProtest['date'], preProtest['close'], label='Jan-Aug 2011', color='red')\n",
        "\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('S&P 500 Index')\n",
        "  plt.title('''S&P 500 Index Comparison: Prior to and During the Time of the 'Occupy Wall Street Movement' ''')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  ##word cloud\n",
        "  with open(\"occupyws.json\", 'r') as f:\n",
        "    news = json.load(f)\n",
        "\n",
        "  occupy_news = [article for article in news]\n",
        "\n",
        "  def scrapeArticle(url):\n",
        "      article = Article(url)\n",
        "      article.download()\n",
        "      article.parse()\n",
        "      return article.text\n",
        "\n",
        "  text = ''\n",
        "  for article in occupy_news:\n",
        "      article_text = scrapeArticle(article['link'])\n",
        "      text += article_text\n",
        "\n",
        "\n",
        "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis('off')\n",
        "  plt.title(\"Word Cloud of Articles about Occupy Wall Street Movement\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def lizette_q2() :\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  data = pd.read_csv('spx.csv')\n",
        "  data['date'] = pd.to_datetime(data['date'])\n",
        "  data.set_index('date', inplace=True)\n",
        "\n",
        "\n",
        "  recent_data = data.loc['2017-09-29':'2018-09-29']\n",
        "  historical_data = data.loc[:'2017-09-29']\n",
        "\n",
        "\n",
        "  past_year_returns = recent_data['close'].pct_change()\n",
        "  historical_returns = historical_data['close'].pct_change()\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(past_year_returns.index, past_year_returns, label='Past Year Returns')\n",
        "  plt.plot(historical_returns.index, historical_returns, label='Historical Returns')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Returns')\n",
        "  plt.title('S&P 500 Index Returns Comparison')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lizette_q3() :\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  ##calculate yearly avg. close data from spf 500\n",
        "  sp500_data = pd.read_csv('spx.csv')\n",
        "  sp500_data['date'] = pd.to_datetime(sp500_data['date'])\n",
        "  sp500_data.set_index('date', inplace=True)\n",
        "  yearly_avg = sp500_data['close'].resample('Y').mean()\n",
        "\n",
        "\n",
        "  # Plot the yearly average close\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(yearly_avg, marker='o', linestyle='-')\n",
        "  plt.title('S&P 500 and Unemployment Rate from 1986-2018')\n",
        "  plt.xlabel('Year')\n",
        "  plt.ylabel('Average Close')\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n",
        "\n",
        "  ##get unemploy. data\n",
        "  unemployment_data = pd.read_csv('USUnemploymentRate.csv')\n",
        "  years = unemployment_data['Year']\n",
        "  unemployment_rates = unemployment_data['AVG']\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(years, unemployment_rates, marker='o', linestyle='-', color='orange')\n",
        "\n",
        "  plt.title('Yearly Average Unemployment Rates in the US (1986-2018)')\n",
        "  plt.xlabel('Year')\n",
        "  plt.ylabel('Unemployment Rate (%)')\n",
        "\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def thieu_q1() :\n",
        "  import yfinance as yf\n",
        "  import matplotlib.pyplot as plt\n",
        "  import os\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import requests\n",
        "  import plotly.graph_objects as go\n",
        "  from plotly.subplots import make_subplots\n",
        "  from bs4 import BeautifulSoup\n",
        "  sp500_symbol = '^GSPC'\n",
        "\n",
        "  sp500_data = yf.Ticker(sp500_symbol).history(period='max')\n",
        "\n",
        "  sp500_data['Year'] = sp500_data.index.year\n",
        "  companies_per_year = sp500_data.groupby('Year').size()\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(companies_per_year.index, companies_per_year.values, marker='o', linestyle='-')\n",
        "  plt.title(\"Number of Companies Listed on the S&P 500 Over the Years\")\n",
        "  plt.xlabel(\"Year\")\n",
        "  plt.ylabel(\"Number of Companies\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(sp500_data['Close'], label='S&P 500')\n",
        "  plt.title('S&P 500 Index')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Price')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def thieu_q2() :\n",
        "  import yfinance as yf\n",
        "  import matplotlib.pyplot as plt\n",
        "  import os\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import requests\n",
        "  import plotly.graph_objects as go\n",
        "  from plotly.subplots import make_subplots\n",
        "  from bs4 import BeautifulSoup\n",
        "\n",
        "  sp500_symbol = '^GSPC'\n",
        "  gold_symbol = 'GC=F'\n",
        "\n",
        "  sp500_data = yf.Ticker(sp500_symbol).history(period='10y')\n",
        "  gold_data = yf.Ticker(gold_symbol).history(period='10y')\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(sp500_data['Close'], label='S&P 500')\n",
        "  plt.plot(gold_data['Close'], label='Gold')\n",
        "  plt.title('S&P 500 Index vs. Gold Price Comparison Over the Years')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Price')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  dir0='coinDataSets'\n",
        "  files=os.listdir(dir0)\n",
        "  paths=[]\n",
        "  for item in files:\n",
        "      paths+=[os.path.join(dir0,item)]\n",
        "      files2=files.copy()\n",
        "\n",
        "  rates=[]\n",
        "  for i in range(len(files)):\n",
        "      data=pd.read_csv(paths[i])\n",
        "      data['MA60']=data['Close'].rolling(window=60).mean()\n",
        "      data['MA60 shift year']=data['MA60'].shift(250)\n",
        "      data['Growth Rate']=(data['MA60']-data['MA60 shift year'])*100/data['MA60 shift year']\n",
        "      files[i]=data\n",
        "      rates+=[[i,files2[i][5:-4],data['Growth Rate'].tolist()[-1]]]\n",
        "\n",
        "  dataGR=pd.DataFrame(columns=['i','Name','Growth Rate %'],data=rates)\n",
        "  dataGR=dataGR.dropna()\n",
        "  dataGR['log growth rate']=dataGR['Growth Rate %'].apply(lambda x:np.log(x))\n",
        "  dataGR2=dataGR.sort_values('Growth Rate %',ascending=False).reset_index(drop=True)\n",
        "  display(dataGR2)\n",
        "\n",
        "  #Plotly format\n",
        "  name0='Bitcoin'\n",
        "  i=dataGR2[dataGR2['Name']==name0]['i'].tolist()[0]\n",
        "  data=files[i]\n",
        "  fig=make_subplots(specs=[[{\"secondary_y\":False}]])\n",
        "  fig.add_trace(go.Scatter(x=data['Date'],y=data['Growth Rate'],name='Growth Rate'),secondary_y=False,)\n",
        "  fig.update_layout(autosize=False,width=800,height=400,title_text=name0+' Growth Rate')\n",
        "  fig.update_xaxes(title_text=\"Date\")\n",
        "  fig.update_yaxes(title_text=\"Growth Rate %\",secondary_y=False)\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "\n",
        "def thieu_q3() :\n",
        "  import yfinance as yf\n",
        "  import matplotlib.pyplot as plt\n",
        "  import os\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import requests\n",
        "  import plotly.graph_objects as go\n",
        "  from plotly.subplots import make_subplots\n",
        "  from bs4 import BeautifulSoup\n",
        "  sp500 = yf.Ticker('^GSPC')\n",
        "  sp500_data = sp500.history(period='10y')\n",
        "\n",
        "  sp500_data['Daily_Return'] = sp500_data['Close'].pct_change()\n",
        "\n",
        "  biggest_losses = sp500_data.nsmallest(20, 'Daily_Return')\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "\n",
        "  plt.plot(sp500_data['Close'], label='S&P 500')\n",
        "\n",
        "  plt.plot(biggest_losses.index, biggest_losses['Close'], 'ro', label='Biggest Losses')\n",
        "\n",
        "  plt.title('S&P 500 Index Over the Last 10 Years with Biggest Losses')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Price')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  sp500 = yf.Ticker('^GSPC')\n",
        "  historical_data = sp500.history(period='max')\n",
        "\n",
        "  historical_data['Daily_Return'] = historical_data['Close'].pct_change()\n",
        "\n",
        "  biggest_losses = historical_data.nsmallest(10, 'Daily_Return')\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.barh(biggest_losses.index.strftime('%Y-%m-%d'), biggest_losses['Daily_Return'], color='red')\n",
        "  plt.xlabel('Daily Percentage Change')\n",
        "  plt.ylabel('Date')\n",
        "  plt.title('Biggest Value Losses in S&P 500')\n",
        "  plt.gca().invert_yaxis()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#MENU\n",
        "\n",
        "repeat = True\n",
        "while repeat == True:\n",
        "    print('\\t---Pythons That Byte Menu---\\n'\n",
        "    \"1) How do closing prices correlate with the S&P 500?\\n\"\n",
        "    \"2) How does the S&P 500 index affect market phases?\\n\"\n",
        "    \"3) How do the market phases correlate withe the CBOE volatility index?\\n\"\n",
        "    \"4) What is the correlation between the CPI and S&P 500?\\n\"\n",
        "    \"5) What is the relationship between the S&P 500 price action and GNP?\\n\"\n",
        "    \"6) What is the relationship between US Crude Oil FPP and S&P 500?\\n\"\n",
        "    \"7) Do presidential administration changes drastically affect the S&P 500?\\n\"\n",
        "    \"8) Historically, what years have the worst yearly change percentage?\\n\"\n",
        "    \"9) What years were the best and the lowest in terms of the S&P 500’s value?\\n\"\n",
        "    \"10) Historically, what year had the highest interests rates, and how did that affect the S&P 500?\\n\"\n",
        "    \"11) Trends in the S&P 500’s performance during the first 100 days of a new presidential administration?\\n\"\n",
        "    \"12) Is there any correlation between the S&P 500 and GDP? If there is, is it negative or positive?\\n\"\n",
        "    \"13) How is the S&P 500 affected during times of protest such as the Occupy Wall Street Movement?\\n\"\n",
        "    \"14) How has the S&P 500 performed in recents times in comparison to how it’s performed historically?\\n\"\n",
        "    \"15) What relationship is there with unemployment rates in the US and the S&P 500 index?\\n\"\n",
        "    \"16) Has there been a significant increase in the number of companies listed on the S&P 500 over the years?\\n\"\n",
        "    \"17) What seem to be relevant to the S&P 500 index prices?\\n\"\n",
        "    \"18) Do crashes lead to significant changes in S&P prices?\\n\"\n",
        "    \"0) Exit\")\n",
        "\n",
        "    choice = input('Enter choice: ')\n",
        "\n",
        "    if choice == \"1\" :\n",
        "      nick_q1()\n",
        "      continue\n",
        "    elif choice == \"2\" :\n",
        "      nick_q2()\n",
        "      continue\n",
        "    elif choice == \"3\" :\n",
        "      nick_q3()\n",
        "      continue\n",
        "    elif choice == \"4\" :\n",
        "      noah_q1()\n",
        "      continue\n",
        "    elif choice == \"5\" :\n",
        "      noah_q2()\n",
        "      continue\n",
        "    elif choice == \"6\" :\n",
        "      noah_q3()\n",
        "      continue\n",
        "    elif choice == \"7\" :\n",
        "      siya_q1()\n",
        "      continue\n",
        "    elif choice == \"8\" :\n",
        "      siya_q2()\n",
        "      continue\n",
        "    elif choice == \"9\" :\n",
        "      siya_q3()\n",
        "      continue\n",
        "    elif choice == \"10\" :\n",
        "      brady_q1()\n",
        "      continue\n",
        "    elif choice == \"11\" :\n",
        "      brady_q2()\n",
        "      continue\n",
        "    elif choice == \"12\" :\n",
        "      brady_q3()\n",
        "      continue\n",
        "    elif choice == \"13\" :\n",
        "      lizette_q1()\n",
        "      continue\n",
        "    elif choice == \"14\" :\n",
        "      lizette_q2()\n",
        "      continue\n",
        "    elif choice == \"15\" :\n",
        "      lizette_q3()\n",
        "      continue\n",
        "    elif choice == \"16\" :\n",
        "      thieu_q1()\n",
        "      continue\n",
        "    elif choice == \"17\" :\n",
        "      thieu_q2()\n",
        "      continue\n",
        "    elif choice == \"18\" :\n",
        "      thieu_q3()\n",
        "      continue\n",
        "    elif choice == \"0\" :\n",
        "      print('Thank you!')\n",
        "      repeat = False\n",
        "    else:\n",
        "      print('Improper input!\\n')\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}